[Data]
train_path = dataset\imdb\train.pkl
dev_path = dataset\imdb\dev.pkl
test_path = dataset\imdb\test.pkl
preload_word_to_idx = true
word_to_idx_path = ulmfit/imdb_stoi.json
label_to_idx_path = dataset/imdb/label_to_idx.json

[Preprocessing]
preprocess_all = false
dataset_name = imdb
write_data_dir = dataset/imdb
restructure_docs = true
balance_dataset = true # only for parsing from sheet (csv/xlsx)
max_seq_len = 100
min_freq_word = 7
percentage_train = 0.8
percentage_dev = 0.0

[Logging]
log_path = log.txt
#class_report_dir = class_reports
tensorboard_dir = runs

[Training]
do_train = true
do_eval = true
save_dir = models
binary_class = false
train_batch_size = 16
eval_batch_size = 32
learning_rate = 0.001
num_cycles_lr = 3 # Number of warm restarts used with cosine annealing schedule
dropout = 0.5 #0.4242576381399609
num_train_epochs = 30
eval_every = 1250
K = 1 #TODO: propagation
weight_decay = 0 #1.0138176410929274e-05
label_value = 1

[Model]
model_name = Hcapsnet
word_encoder = ulmfit
sent_encoder = gru
use_glove = false
embed_dim = 300
word_hidden = 100
sent_hidden = 200

[CapsNet]
dim_caps = 16
num_caps = 512
num_compressed_caps = 200
num_head_doc = 2
dropout_caps = 0.35
lambda_reg_caps = 0.0003408222329810838
KDE_epsilon = 0.05

[FastText]
word_vec_path = word vectors/custom_imdb.bin
create_wordvecs = false
use_ft_baseline = false
ft_save_path = fasttext.model
ft_n_epoch = 50
ft_minn = 3 # char ngrams
ft_maxn = 6 # char ngrams
ft_lr = 0.05 # learning rate

[ULMFiT]
create_doc_encodings = false
ulmfit_pretrained_path = ulmfit/imdb_lm15.pth #ulmfit/lm_torch3.pt #
dropout_factor = 1.5
lr_div_factor = 5 # factor to divide original lr by to finetune ulmfit
gradual_unfreeze = false
keep_frozen = false

