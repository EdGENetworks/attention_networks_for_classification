[DataAndPreprocessing]
train_path = dataset/reuters-full/train.pkl
dev_path = dataset/reuters-full/test.pkl
test_path = dataset/reuters-full/test.pkl
write_data_dir = dataset/reuters-full
word_vec_path = word vectors/custom.bin
preload_word_to_idx = false
word_to_idx_path = dataset/reuters-full/word_to_idx.json #reuters_stoi.json #   ulmfit_word_to_idx.json # #
label_to_idx_path = dataset/reuters-full/label_to_idx.json
min_freq_word = 15
max_seq_len = 100

[Training]
train_batch_size = 16
eval_batch_size = 16
learning_rate = 0.002
dropout = 0.6273196318562873
num_train_epochs = 20
eval_every = 500
K = 1 #TODO: propagation
weight_decay = 0.0010433743805458922
binary_class = true #TODO: propagation
label_value = 0.9

[Model]
model_name = HCapsNet
word_encoder = gru
sent_encoder = gru
use_glove = false
embed_dim = 300
word_hidden = 100
sent_hidden = 200

[CapsNet]
dim_caps = 16
num_caps = 32
num_compressed_caps = 170
num_head_doc = 5
dropout_caps = 0.5259016807172748
lambda_reg_caps = 1.9769541881597107e-05

[FastText]
create_wordvecs = false

[ULMFiT]
ulmfit_pretrained_path = ulmfit/reuters_lm15.pth #ulmfit/lm_torch3.pt #
dropout_factor = 2.5

