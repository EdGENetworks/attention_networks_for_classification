[DataAndPreprocessing]
preprocess_all = false
train_path = dataset/twentynews/train.pkl
dev_path = dataset/twentynews/dev.pkl
test_path = dataset/twentynews/test.pkl
write_data_dir = dataset/twentynews
word_vec_path = word vectors/custom.bin
preload_word_to_idx = false
word_to_idx_path = dataset/twentynews/stoi1.json #dataset/reuters/stoi.json #ulmfit/imdb_stoi.json   ulmfit_word_to_idx.json #dataset/reuters/word_to_idx.json #dataset/imdb/stoi.json
label_to_idx_path = dataset/twentynews/label_to_idx.json
min_freq_word = 41
dataset_name = 20news
max_seq_len = 100
percentage_train = 0.8
percentage_dev = 0.0

[Training]
binary_class = false
train_batch_size = 16
eval_batch_size = 32
learning_rate = 0.0025
dropout = 0.3163139487152957
num_train_epochs = 30
eval_every = 565
K = 1 #TODO: propagation
weight_decay = 2.43624658089963e-05
label_value = 0.9680114815754679

[Model]
model_name = Hcapsnet
word_encoder = gru
sent_encoder = gru
use_glove = false
embed_dim = 300
word_hidden = 100
sent_hidden = 100

[CapsNet]
dim_caps = 16
num_caps = 32
num_compressed_caps = 50
num_head_doc = 5
dropout_caps = 0.14159837139688117
lambda_reg_caps = 9.373308883264298e-06

[FastText]
create_wordvecs = false
use_ft_baseline = false
ft_save_path = fasttext.model
ft_n_epoch = 25
ft_minn = 3 # char ngrams
ft_maxn = 6 # char ngrams
ft_lr = 0.05 # learning rate

[ULMFiT]
create_doc_encodings = false
ulmfit_pretrained_path = ulmfit/reuters_lm15.1.pth #ulmfit/lm_torch3.pt #
dropout_factor = 1.2168930640915903
gradual_unfreeze = false
keep_frozen = false #TODO: check logic of both flags

